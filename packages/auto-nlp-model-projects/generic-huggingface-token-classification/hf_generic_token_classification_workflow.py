#!/usr/bin/env python

# This script has been built on top of https://github.com/huggingface/transformers/blob/0d1f67e651220bffef1441fa7589620e426ba958/examples/pytorch/token-classification/run_ner.py
# Copyright claim from original source:
# coding=utf-8
# Copyright 2020 The HuggingFace Team All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Fine-tuning the library models for token classification.
"""
# You can also adapt this script on your own token classification task and datasets. Pointers for this are left as
# comments.

import logging
import os
from dataclasses import dataclass, field
from glob import glob
from typing import Dict, Optional, Tuple

import numpy as np
import sys
import transformers
from datasets import ClassLabel, DatasetDict, load_metric
from transformers import (
    AutoConfig,
    AutoModelForTokenClassification,
    AutoTokenizer,
    DataCollatorForTokenClassification,
    PreTrainedTokenizerFast,
    Trainer,
    TrainingArguments,
    set_seed
)
from transformers.trainer_utils import is_main_process

# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
from tagflip.model import AutoNLPArguments
from tagflip.model.autonlp_workflow import Args, ArgsType
from tagflip.model.mlflow.huggingface import HuggingFaceTokenClassificationSavable, HuggingFaceWorkflow

logger = logging.getLogger(__name__)


@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
    """
    model_name: str = field(
            metadata={"help": "Name of pretrained model or model identifier from huggingface.co/models"}
    )
    config_name: Optional[str] = field(
            default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
            default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    model_revision: str = field(
            default="main",
            metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    use_auth_token: bool = field(
            default=False,
            metadata={
                "help": "Will use the token generated when running `transformers-cli login` (necessary to use this script "
                        "with private models)."
            },
    )


@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """
    task_name: Optional[str] = field(default="ner", metadata={"help": "The name of the task (ner, pos...)."})
    pad_to_max_length: bool = field(
            default=False,
            metadata={
                "help": "Whether to pad all samples to model maximum sentence length. "
                        "If False, will pad the samples dynamically when batching to the maximum length in the batch. More "
                        "efficient on GPU but very bad for TPU."
            },
    )
    max_train_samples: Optional[int] = field(
            default=None,
            metadata={
                "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
                        "value if set."
            },
    )
    max_val_samples: Optional[int] = field(
            default=None,
            metadata={
                "help": "For debugging purposes or quicker training, truncate the number of validation examples to this "
                        "value if set."
            },
    )
    max_test_samples: Optional[int] = field(
            default=None,
            metadata={
                "help": "For debugging purposes or quicker training, truncate the number of test examples to this "
                        "value if set."
            },
    )
    label_all_tokens: bool = field(
            default=False,
            metadata={
                "help": "Whether to put the label for one word on all tokens of generated by that word or just on the "
                        "one (in which case the other tokens will have a padding index)."
            },
    )
    return_entity_level_metrics: bool = field(
            default=False,
            metadata={"help": "Whether to return all the entity levels during evaluation or just the overall ones."},
    )

    def __post_init__(self):
        self.task_name = self.task_name.lower()


@dataclass
class HyperSpaceTuneArgs:
    trials: int = field(default=1, metadata={"help": "The number of trials that should be performed."})
    learning_rate_min_max: Tuple[float, float] = field(default=(1e-6, 1e-4), metadata={
        "help": "The minimum and maximum value to sample from uniform distribution of given ranges"})

    max_num_train_epochs: int = field(default=1, metadata={"help": "The maximum number of train epochs"})

    seed_max: int = field(default=40, metadata={
        "help": "The maximum value to sample from uniform distribution of range [1,seed_max]"})

    possible_train_batch_sizes: Tuple[int] = field(default=(4, 8, 16, 32, 64), metadata={
        "help": "Possible batch sizes to choose from."})


class HFGenericTokenClassificationWorkflow(HuggingFaceWorkflow):

    def __init__(self):
        super().__init__((TrainingArguments, ModelArguments, DataTrainingArguments), HyperSpaceTuneArgs)

    def train(self, datasets: DatasetDict, autonlp_args: AutoNLPArguments, args: Dict[ArgsType, Args],
              hyperspace_tune_args: HyperSpaceTuneArgs = None):
        training_args = args[TrainingArguments]
        model_args = args[ModelArguments]
        data_args = args[DataTrainingArguments]

        train_dataset = datasets['train']
        valid_dataset = datasets['validation']
        test_dataset = datasets['test']

        # Setup logging
        logging.basicConfig(format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
                            datefmt="%m/%d/%Y %H:%M:%S",
                            handlers=[logging.StreamHandler(sys.stdout)])
        logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)

        # Log on each process the small summary:
        logger.warning(
                f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
                + f"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
        )
        # Set the verbosity to info of the Transformers logger (on main process only):
        if is_main_process(training_args.local_rank):
            transformers.utils.logging.set_verbosity_info()
            transformers.utils.logging.enable_default_handler()
            transformers.utils.logging.enable_explicit_format()
        logger.info(f"Training/evaluation parameters {training_args}")

        # Set seed before initializing model.
        set_seed(training_args.seed)

        column_names = train_dataset.column_names
        features = train_dataset.features

        text_column_name = "tokens" if "tokens" in column_names else column_names[0]
        label_column_name = (
            f"{data_args.task_name}_tags" if f"{data_args.task_name}_tags" in column_names else column_names[1]
        )

        # In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the
        # unique labels.
        def get_label_list(labels):
            unique_labels = set()
            for label in labels:
                unique_labels = unique_labels | set(label)
            label_list = list(unique_labels)
            label_list.sort()
            return label_list

        if isinstance(features[label_column_name].feature, ClassLabel):
            label_list = features[label_column_name].feature.names
            # No need to convert the labels since they are already ints.
            label_to_id = {i: i for i in range(len(label_list))}
        else:
            label_list = get_label_list(train_dataset[label_column_name])
            label_to_id = {l: i for i, l in enumerate(label_list)}
        num_labels = len(label_list)

        # Load pretrained model and tokenizer
        #
        # Distributed training:
        # The .from_pretrained methods guarantee that only one local process can concurrently
        # download model & vocab.
        config = AutoConfig.from_pretrained(
                model_args.config_name if model_args.config_name else model_args.model_name,
                num_labels=num_labels,
                finetuning_task=data_args.task_name,
                revision=model_args.model_revision,
                use_auth_token=True if model_args.use_auth_token else None,
        )
        tokenizer = AutoTokenizer.from_pretrained(
                model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name,
                use_fast=True,
                revision=model_args.model_revision,
                use_auth_token=True if model_args.use_auth_token else None,
        )

        def model_init():
            return AutoModelForTokenClassification.from_pretrained(
                    model_args.model_name,
                    # from_tf=bool(".ckpt" in model_args.model_name),
                    config=config,
                    revision=model_args.model_revision,
                    use_auth_token=True if model_args.use_auth_token else None,
                    # return_dict=True
            )

        # Tokenizer check: this script requires a fast tokenizer.
        if not isinstance(tokenizer, PreTrainedTokenizerFast):
            raise ValueError(
                    "This script only works for models that have a fast tokenizer. Checkout the big table of models "
                    "at https://huggingface.co/transformers/index.html#bigtable to find the model types that meet this "
                    "requirement"
            )

        # Preprocessing the dataset
        # Padding strategy
        padding = "max_length" if data_args.pad_to_max_length else False

        # Tokenize all texts and align the labels with them.
        def tokenize_and_align_labels(examples):
            tokenized_inputs = tokenizer(
                    examples[text_column_name],
                    padding=padding,  # whether to pad to max seq length or not
                    truncation=True,
                    # We use this argument because the texts in our dataset are lists of words (with a label for each word).
                    is_split_into_words=True,
            )
            labels = []
            for i, label in enumerate(examples[label_column_name]):
                word_ids = tokenized_inputs.word_ids(batch_index=i)
                previous_word_idx = None
                label_ids = []
                for word_idx in word_ids:
                    # Special tokens have a word id that is None. We set the label to -100 so they are automatically
                    # ignored in the loss function.
                    if word_idx is None:
                        label_ids.append(-100)
                    # We set the label for the first token of each word.
                    elif word_idx != previous_word_idx:
                        label_ids.append(label_to_id[label[word_idx]])
                    # For the other tokens in a word, we set the label to either the current label or -100, depending on
                    # the label_all_tokens flag.
                    else:
                        label_ids.append(label_to_id[label[word_idx]] if data_args.label_all_tokens else -100)
                    previous_word_idx = word_idx

                labels.append(label_ids)
            tokenized_inputs["labels"] = labels
            return tokenized_inputs

        if data_args.max_train_samples is not None:
            train_dataset = train_dataset.select(range(data_args.max_train_samples))
        train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)

        if data_args.max_val_samples is not None:
            valid_dataset = valid_dataset.select(range(data_args.max_val_samples))
        valid_dataset = valid_dataset.map(tokenize_and_align_labels, batched=True)

        test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)

        # Data collator
        data_collator = DataCollatorForTokenClassification(tokenizer,
                                                           pad_to_multiple_of=8 if training_args.fp16 else None)

        # Metrics
        metric = load_metric("seqeval")

        def compute_metrics(p):
            predictions, labels = p
            predictions = np.argmax(predictions, axis=2)

            # Remove ignored index (special tokens)
            true_predictions = [
                [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
                for prediction, label in zip(predictions, labels)
            ]
            true_labels = [
                [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
                for prediction, label in zip(predictions, labels)
            ]

            results = metric.compute(predictions=true_predictions, references=true_labels)
            if data_args.return_entity_level_metrics:
                # Unpack nested dictionaries
                final_results = {}
                for key, value in results.items():
                    if isinstance(value, dict):
                        for n, v in value.items():
                            final_results[f"{key}_{n}"] = v
                    else:
                        final_results[key] = value
                return final_results
            else:
                return {
                    "precision": results["overall_precision"],
                    "recall": results["overall_recall"],
                    "f1": results["overall_f1"],
                    "accuracy": results["overall_accuracy"],
                }

        trainer = Trainer(
                model_init=model_init,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=valid_dataset,
                tokenizer=tokenizer,
                data_collator=data_collator,
                compute_metrics=compute_metrics,
        )
        trainer.add_callback(self.get_hf_tagflip_trainer_callback())  # this is important to attach tagflip capabilities

        logger.info("*** Evaluate untrained ***")
        metrics = trainer.evaluate()
        max_val_samples = data_args.max_val_samples if data_args.max_val_samples is not None else len(valid_dataset)
        metrics["eval_samples"] = min(max_val_samples, len(valid_dataset))
        trainer.log_metrics("eval", metrics)
        self.log_metrics(metrics)

        if hyperspace_tune_args:
            def hp_space_fn(trial) -> Dict[str, float]:
                # return {
                #     "learning_rate": tune.loguniform(hyperspace_tune_args.learning_rate_min_max[0],
                #                                      hyperspace_tune_args.learning_rate_min_max[1]),
                #     "num_train_epochs": tune.choice(list(range(1, hyperspace_tune_args.max_num_train_epochs))),
                #     "seed": tune.uniform(1, hyperspace_tune_args.seed_max),
                #     "per_device_train_batch_size": tune.choice(list(hyperspace_tune_args.possible_train_batch_sizes)),
                # }
                return {
                    "learning_rate": trial.suggest_float("learning_rate", hyperspace_tune_args.learning_rate_min_max[0],
                                                         hyperspace_tune_args.learning_rate_min_max[1], log=True),
                    "num_train_epochs": trial.suggest_int("num_train_epochs", 1,
                                                          hyperspace_tune_args.max_num_train_epochs),
                    "seed": trial.suggest_int("seed", 1, hyperspace_tune_args.seed_max),
                    "per_device_train_batch_size": trial.suggest_categorical("per_device_train_batch_size", list(
                            hyperspace_tune_args.possible_train_batch_sizes)),
                }

            logger.info(
                    f"Starting training with automatic hyperparameter search. There will be {hyperspace_tune_args.trials} trials.")
            best_run = trainer.hyperparameter_search(hp_space_fn, backend="optuna",
                                                     compute_objective=lambda x: x['eval_f1'],
                                                     direction="maximize",
                                                     n_trials=hyperspace_tune_args.trials)
            model_dir = os.path.join(training_args.output_dir, f"run-{best_run.run_id}")
            logger.info(f"Loading best model from {model_dir}.")
            last_checkpoint_dir = sorted(glob(os.path.join(model_dir, "*", "")))[-1]
            model = AutoModelForTokenClassification.from_pretrained(last_checkpoint_dir)
            tmp_trainer = Trainer(
                    model=model,
                    args=training_args,
                    train_dataset=train_dataset,
                    eval_dataset=valid_dataset,
                    tokenizer=tokenizer,
                    data_collator=data_collator,
                    compute_metrics=compute_metrics,
            )
            tmp_trainer.add_callback(
                self.get_hf_tagflip_trainer_callback())  # this is important to attach tagflip capabilities
            logger.info("*** Evaluate best found model ***")
            metrics = tmp_trainer.evaluate()
            max_val_samples = data_args.max_val_samples if data_args.max_val_samples is not None else len(valid_dataset)
            metrics["eval_samples"] = min(max_val_samples, len(valid_dataset))
            tmp_trainer.log_metrics("eval", metrics)
            self.log_metrics(metrics)

            # Test
            logger.info("*** Predict ***")
            metrics = tmp_trainer.predict(test_dataset)[2]
            tmp_trainer.log_metrics("test", metrics)
            self.log_metrics(metrics)

            return HuggingFaceTokenClassificationSavable(trainer=tmp_trainer, label_list=label_list)
        else:
            # Training
            logger.info("Starting training")
            train_result = trainer.train()
            metrics = train_result.metrics
            max_train_samples = (
                data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
            )
            metrics["train_samples"] = min(max_train_samples, len(train_dataset))
            trainer.log_metrics("train", metrics)
            self.log_metrics(metrics)
            trainer.save_state()

            # Test
            logger.info("*** Predict ***")
            metrics = trainer.predict(test_dataset)[2]
            trainer.log_metrics("test", metrics)
            self.log_metrics(metrics)

            return HuggingFaceTokenClassificationSavable(trainer=trainer, label_list=label_list)


def _mp_fn(index):
    # For xla_spawn (TPUs)
    HFGenericTokenClassificationWorkflow().run()


if __name__ == '__main__':
    HFGenericTokenClassificationWorkflow().run()
